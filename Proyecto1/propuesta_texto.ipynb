{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4034c3d-c104-43b2-91ec-5302cbdf6bb7",
   "metadata": {},
   "source": [
    "# Propuesta Dataset texto\n",
    "\n",
    "**Datasets seleccionado:** Arxiv NLP (MaartenGr)\n",
    "\n",
    "**URL:** https://huggingface.co/datasets/MaartenGr/arxiv_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b4eb0b-218b-4c96-af8f-ad226803604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado correctamente\n",
      "Filas: 44949, Columnas: 4\n",
      "                                              Titles  \\\n",
      "0  Introduction to Arabic Speech Recognition Usin...   \n",
      "1  Arabic Speech Recognition System using CMU-Sph...   \n",
      "2  On the Development of Text Input Method - Less...   \n",
      "3  Network statistics on early English Syntax: St...   \n",
      "4  Segmentation and Context of Literary and Music...   \n",
      "\n",
      "                                           Abstracts  Years  \\\n",
      "0    In this paper Arabic was investigated from t...   2007   \n",
      "1    In this paper we present the creation of an ...   2007   \n",
      "2    Intelligent Input Methods (IM) are essential...   2007   \n",
      "3    This paper includes a reflection on the role...   2007   \n",
      "4    We test a segmentation algorithm, based on t...   2007   \n",
      "\n",
      "                 Categories  \n",
      "0  Computation and Language  \n",
      "1  Computation and Language  \n",
      "2  Computation and Language  \n",
      "3  Computation and Language  \n",
      "4  Computation and Language  \n"
     ]
    }
   ],
   "source": [
    "# Librerías\n",
    "import pandas as pd\n",
    "\n",
    "# URL directa al CSV en Hugging Face\n",
    "url = \"https://huggingface.co/datasets/MaartenGr/arxiv_nlp/resolve/main/data.csv\"\n",
    "\n",
    "# Cargar directamente en un DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Mostrar información básica\n",
    "print(\"Dataset cargado correctamente\")\n",
    "print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5a755-9ac8-4fac-944e-7b226a04957a",
   "metadata": {},
   "source": [
    "**1. ¿Qué problema concreto resolverás? ¿Por qué es relevante?**\n",
    "\n",
    "Para comenzar, este dataset contiene metadatos y resúmenes de artículos de arXiv (una plataforma de investigación científica).<br><br>\n",
    "El problema concreto que resolvería es la clasificación y análisis de artículos científicos según su contenido.<br>\n",
    "Para aquello principalmente hay que buscar resolver, cómo entender, organizar y clasificar automáticamente estos textos mediante técnicas de Procesamiento del Lenguaje Natural (NLP).<br>\n",
    "\n",
    "Por qué es relevante?\n",
    "\n",
    "**1. Volumen creciente de investigación:** Al haber miles de papers nuevos publicad por día en arXiv. Clasificarlos y analizarlos manualmente es inviable.<br>\n",
    "**2. Automatización inteligente:** Permitiría entrenar modelos como, la clasificación de artículos por tema, la generación de resúmenes automátizados, la detección de similitudes entre trabajos y la busqueda de tendencias emergentes en la investigación científica.<br>\n",
    "**3. Aplicaciones prácticas:** la creación de motores de recomendación de papers, sistemas de búsqueda semántica (por contenido, no solo por palabras claves) y el análisis de tendencias en ciencia (por ejemplo, detectar áreas de rápido crecimiento).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fd2d8-6a10-4056-bb14-b9565f4537d4",
   "metadata": {},
   "source": [
    "**¿Qué tipo de modelo de aprendizaje no supervisado usarás inicialmente? escoger al menos 2 vistos en clase y 2 no vistos en clase? Explicar teóricamente como funciona el modelo y los algoritmos usados por cada método.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f5981-11b9-4a18-85c1-7fee90a0b850",
   "metadata": {},
   "source": [
    "Para el análisis del dataset Arxiv NLP se propone aplicar cuatro modelos de aprendizaje no supervisado, combinando dos métodos vistos en clase y dos no vistos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473843b8-71a4-4b4f-9fbe-25c32ca4060c",
   "metadata": {},
   "source": [
    "**Modelos vistos en clase**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5317812-1038-4c3b-839c-94333c9790e9",
   "metadata": {},
   "source": [
    "**K-means**\n",
    "- K-means en un dataset de texto funciona primero transformando los documentos en vectores numéricos, típicamente mediante TF-IDF o embeddings que representan el contenido de cada texto.<br>\n",
    "- Luego se elige un número de clusters k y se inicializan k centroides aleatorios.<br>\n",
    "- A cada documento se asigna al centroide más cercano según una medida de distancia, después se recalculan los centroides como el promedio de los vectores asignados a cada cluster.<br>\n",
    "- Repitiendo este proceso hasta que las asignaciones de documentos ya no cambian significativamente, de modo que los textos queden agrupados en clusters que minimizan la variación interna dentro de cada grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5f8ca-131c-4b5b-8a5c-270114d0fefc",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32af7c-b2dc-42f6-8d10-ba678f8687e0",
   "metadata": {},
   "source": [
    "- Es una técnica de reducción de dimensionalidad que simplifica conjuntos de datos complejos y de alta dimensión.<br>\n",
    "- Funciona transformando un conjunto de variables originales correlacionadas en un conjunto más pequeño de variables nuevas (componentes principales) que conservan la mayor parte de la información.<br>\n",
    "- Se utiliza para preprocesar datos, explorar datos, visualizar tendencias y patrones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61f9c0-66e6-48a4-a26e-cdcd478771f8",
   "metadata": {},
   "source": [
    "**Modelos no vistos en clase**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a91eff6-af5d-4e9f-8c36-cbb43bd0c574",
   "metadata": {},
   "source": [
    "**Spectral Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e2ba4-9698-46b0-8ecc-a45985f283f2",
   "metadata": {},
   "source": [
    "- Spectral Clustering en un dataset de texto funciona transformando primero los documentos en vectores numéricos (por ejemplo, mediante TF-IDF o embeddings).<br>\n",
    "- Luego construyendo una matriz de similitud que mide qué tan parecidos son los textos entre sí.<br>\n",
    "- A partir de esta matriz se crea un Laplaciano del grafo de similitud y se calculan los vectores propios correspondientes a los valores propios más pequeños, los cuales representan los documentos en un espacio reducido donde los clusters son más claros. <br>\n",
    "- Y finalmente se aplica un algoritmo como k-means sobre este espacio para agrupar los textos según patrones de similitud complejos que no necesariamente son lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43f605-9598-428a-a3f1-425b691cf060",
   "metadata": {},
   "source": [
    "**Hierarchical clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbcab0-1361-4c40-83d7-17e4df486ac1",
   "metadata": {},
   "source": [
    "- Hierarchical clustering en un dataset de texto consiste en primero transformar los documentos en vectores numéricos mediante técnicas como TF-IDF o embeddings, de modo que cada texto quede representado en un espacio vectorial.<br>\n",
    "- Luego, el algoritmo agrupa los documentos de manera jerárquica midiendo su similitud con distancias como la euclidiana o coseno, ya sea de manera aglomerativa (empezando con cada documento como su propio clúster y fusionando progresivamente los más similares) o divisiva (empezando con todos los documentos juntos y dividiéndolos iterativamente).<br>\n",
    "- El resultado se puede visualizar con un dendrograma, que muestra cómo los textos se van uniendo en distintos niveles de similitud, permitiendo identificar clústeres naturales sin definir de antemano su número."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8a09a-0529-4c49-9a1b-a7a9a28e0d7c",
   "metadata": {},
   "source": [
    "**¿Qué métrica de evaluación utilizarás y por qué?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4f28e-1e62-4878-9c9c-2cc7ecb536e1",
   "metadata": {},
   "source": [
    "En un dataset de texto con K-means, la métrica de evaluación más común es el Silhouette Score, porque mide qué tan bien separados y compactos están los clústeres sin necesitar etiquetas reales.<br>\n",
    "El Silhouette Score combina dos factores:<br>\n",
    "- Cohesión: qué tan cerca están los textos dentro de un mismo clúster.\n",
    "- Separación: qué tan lejos están de los textos de otros clústeres.<br>\n",
    "  \n",
    "Su valor va de -1 a 1, donde valores cercanos a 1 indican clústeres bien definidos.\n",
    "Esta métrica es ideal en aprendizaje no supervisado, como en el caso de K-means aplicado a texto, ya que no se requieren clases verdaderas para calcularla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f8431-9e7e-48ef-9555-6be45c010297",
   "metadata": {},
   "source": [
    "**¿Quiénes serían los usuarios o beneficiarios del modelo?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3137b2c-c01a-473b-b231-c292c7cca587",
   "metadata": {},
   "source": [
    "Los usuarios o beneficiarios de un modelo K-means aplicado a un dataset de texto serían principalmente:<br>\n",
    "\n",
    "- Investigadores y analistas de datos, que pueden descubrir temas o patrones ocultos en grandes colecciones de documentos sin necesidad de etiquetarlos previamente.\n",
    "- Empresas o medios de comunicación, que pueden agrupar noticias, reseñas o comentarios de usuarios según su contenido para mejorar la segmentación o el análisis de tendencias.\n",
    "- Instituciones académicas o científicas, que pueden organizar automáticamente artículos, resúmenes o publicaciones por áreas temáticas.\n",
    "- Departamentos de atención al cliente o marketing, que pueden clasificar opiniones o feedback en grupos similares para identificar problemas o intereses comunes.<br>\n",
    "\n",
    "En general, cualquier persona u organización que necesite organizar, explorar o resumir grandes volúmenes de texto se beneficia directamente del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae63f8-d2d9-491a-992f-2ecdba4d5234",
   "metadata": {},
   "source": [
    "**¿Qué esperas lograr con tu modelo? (clasificar, predecir, segmentar, etc.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef03ff-af8b-4a57-be08-9e7b9044127f",
   "metadata": {},
   "source": [
    "Con un modelo K-means aplicado a un dataset de texto, se espera segmentar o agrupar los documentos en función de su similitud de contenido.<br\n",
    "El objetivo no es clasificar ni predecir etiquetas conocidas, sino descubrir patrones ocultos y organizar automáticamente los textos en grupos temáticos que compartan palabras o significados similares.<br> \n",
    "En otras palabras, el modelo busca identificar estructuras naturales dentro del texto, facilitando la exploración, el análisis de temas y la comprensión general del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e701298-5679-4a5c-9bdd-eb2890362fbe",
   "metadata": {},
   "source": [
    "**¿Qué herramientas planeas usar? (PyTorch, TensorFlow, Keras, etc.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51bd12-8a09-413c-969a-41f7314312aa",
   "metadata": {},
   "source": [
    "Para un modelo K-means aplicado a texto, las herramientas más adecuadas y eficientes serían principalmente del ecosistema Python, especialmente:\n",
    "- **scikit-learn** → para implementar el algoritmo K-means, calcular métricas de evaluación (como Silhouette Score) y manejar el flujo de clustering.\n",
    "- **pandas** → para la carga, limpieza y manejo estructurado del dataset de texto.\n",
    "- **scikit-learn.feature_extraction.text (TF-IDF Vectorizer)** o **spaCy / sentence-transformers** → para convertir los textos en vectores numéricos (representación vectorial).\n",
    "- **matplotlib** o seaborn → para visualizar los clústeres y resultados.\n",
    "\n",
    "En este caso no es necesario usar PyTorch, TensorFlow o Keras, ya que K-means no es una red neuronal, sino un algoritmo clásico de aprendizaje no supervisado, que se ejecuta de forma más ligera con scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c19cd-4a44-4081-94d1-3bc9f8f9e090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
